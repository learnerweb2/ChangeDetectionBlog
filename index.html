<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Infrastructure Change Detection Using Satellite Imagery</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1b542ab5-d599-80db-b95e-f02a5252f932" class="page serif"><header><img class="page-cover-image" src="360_F_500840525_z0BVvB9i29AlWU8ZdMtFH0GewrLyBSob.jpg" style="object-position:center 48.870000000000005%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">üõ∞Ô∏è</span></div><h1 class="page-title">Infrastructure Change Detection Using Satellite Imagery</h1><p class="page-description"></p></header><div class="page-body"><p id="1b542ab5-d599-8142-8370-eb2d95818e66" class="block-color-gray">Team 117        -        Guide: MD. Sallaudin Sir</p><p id="1b542ab5-d599-804f-9c47-eea14c739fd2" class="">
</p><p id="1b542ab5-d599-8176-bef5-c3494939739c" class="block-color-gray"><mark class="highlight-default">THIPPANI VENKATA NAGASHESHU MANI - 2103A52036</mark></p><p id="1b542ab5-d599-8039-b940-d79d4ec1b98e" class="">UGGE REETHUVARMA                                              - 2103A52038</p><p id="1b542ab5-d599-80b8-9f73-d88f8261a821" class="">SATTU SAI PRANEETH                                               - 2103A52034 ¬†</p><p id="1b542ab5-d599-805a-9c0d-f966c7c136d3" class="">
</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1b542ab5-d599-8162-902a-c94ef6ff7237"><div style="font-size:1.5em"><span class="icon">üîñ</span></div><div style="width:100%">&quot;Deep learning-based infrastructure change detection using satellite imagery for accurate, automated monitoring and analysis of urban and rural developments.‚Äù</div></figure><h1 id="1b542ab5-d599-80c4-8cc0-ec8c9845a2e8" class="">Introduction</h1><p id="1b542ab5-d599-81a5-b73a-ec6687da23d3" class="">Infrastructure change detection using satellite imagery is a powerful tool that helps monitor changes in buildings, roads, and other structures over time. By using images captured by satellites, we can observe and track how areas change, whether due to construction, natural disasters, or urban development. This kind of analysis is important for urban planning, disaster response, environmental monitoring, and even assessing the impact of climate change.</p><div id="1b542ab5-d599-8131-a3ec-fa6f1d3e560c" class="column-list"><div id="1b542ab5-d599-81b7-8ed0-e3d07609f99f" style="width:50%" class="column"><p id="1b542ab5-d599-80a6-9fe8-efbde6dfde8b" class="">Deep learning, a type of artificial intelligence, plays a big role in this process. It involves training computer models to recognize patterns in satellite images and automatically detect changes. These models learn to differentiate between old and new buildings, roads, and other structures by analyzing large amounts of image data. This means the system can detect even small changes, such as a new building or road that was built recently.</p><p id="1b542ab5-d599-80a8-b769-ed1e97c1a9cc" class="">
</p></div><div id="1b542ab5-d599-813a-ab5a-d25b397ecede" style="width:50%" class="column"><figure id="1b542ab5-d599-81e4-8379-e350b91ff329" class="image"><a href="3419993.png"><img style="width:240px" src="3419993.png"/></a><figcaption>Satellite Imagery</figcaption></figure><p id="1b542ab5-d599-81f7-82d3-ceee98edbae7" class="block-color-gray">
</p></div></div><p id="1b542ab5-d599-819b-9511-cce5878c6e93" class="">Using deep learning for infrastructure change detection is faster and more accurate than traditional methods, which usually involve human analysis of images. With AI, we can detect changes in real-time and make important decisions quickly, like planning for new developments or responding to a disaster. This technology is becoming increasingly important as cities grow and as we need to monitor more areas than ever before.</p><p id="1b542ab5-d599-818b-a371-c72f68a97b22" class=""><br/>In this article, we will discuss four deep learning-based models for change detection: <br/><strong>STANet</strong>, <strong>UNet</strong>, <strong>UNet with ResNet</strong>, and <strong>LCDNet</strong>. Each of these models is designed to improve the performance of change detection tasks by leveraging different techniques for feature extraction, skip connections, and fusion of multi-temporal information.</p><h2 id="1b542ab5-d599-808c-a9c0-fa7966fd991d" class="">1. STANet: Spatio-Temporal Attention Network</h2><p id="1b542ab5-d599-8063-9d61-cc6ce352e720" class="">STANet (Spatio-Temporal Attention Network) is a deep learning model specifically designed for change detection in remote sensing images. Unlike traditional methods, STANet uses an attention mechanism to focus on the most important spatial and temporal features in satellite images. This attention mechanism allows the model to capture both spatial features (such as buildings or roads) and temporal features (such as changes between two time points) in an image.</p><p id="1b542ab5-d599-8078-8daa-fcb686e3f1b2" class="">STANet operates by combining convolutional layers with attention modules that learn which parts of the image are most relevant for detecting changes. These attention modules allow the network to focus on regions where changes are likely to occur while ignoring irrelevant areas. For instance, in urban environments, the network might focus on newly constructed buildings or roads, while ignoring stable areas like forests or rivers that are unlikely to change.</p><p id="1b542ab5-d599-800b-9b45-cf7d87ca82cc" class="">One of the key benefits of STANet is its ability to handle the spatial and temporal dimensions of change detection separately. The model first extracts spatial features from the individual images and then applies a temporal attention mechanism to model the differences between two time points. By doing so, STANet can better capture changes in the scene over time and achieve better accuracy in change detection tasks.</p><h3 id="1b542ab5-d599-8070-8522-e83e6dd37a53" class="">Key Components of STANet Architecture</h3><ol type="1" id="1b542ab5-d599-80d6-a9ad-e5fa4dedb24a" class="numbered-list" start="1"><li><strong>Input Images</strong>:<ul id="1b542ab5-d599-8008-a160-dabf1992c3d1" class="bulleted-list"><li style="list-style-type:disc">The network takes a pair of remote sensing images as input: one representing the earlier time point (reference image) and the other representing the later time point (target image).</li></ul><ul id="1b542ab5-d599-8057-9512-e0db3bc00273" class="bulleted-list"><li style="list-style-type:disc">These images are typically preprocessed to align spatially (e.g., through image registration) and normalized to ensure consistency.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-8040-9d56-e524175faf0a" class="numbered-list" start="2"><li><strong>Feature Extraction (Convolutional Layers)</strong>:<ul id="1b542ab5-d599-805d-9be6-d95c8879fed9" class="bulleted-list"><li style="list-style-type:disc">In the first stage of the architecture, the network extracts high-level features from the input images using convolutional neural networks (CNNs).</li></ul><ul id="1b542ab5-d599-80fa-9c48-c9a120ec3b0d" class="bulleted-list"><li style="list-style-type:disc">The feature extractor uses multiple convolutional layers to capture a hierarchy of features, ranging from low-level features (such as edges) to high-level semantic features (such as buildings, roads, etc.).</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80a9-aea8-fcc6f43b270e" class="numbered-list" start="3"><li><strong>Spatio-Temporal Attention Modules</strong>:<ul id="1b542ab5-d599-80c1-9188-d88ddd2fb69a" class="bulleted-list"><li style="list-style-type:disc">The core innovation of STANet lies in its <strong>spatio-temporal attention mechanism</strong>, which is designed to emphasize regions of the image that show significant changes over time.</li></ul><ul id="1b542ab5-d599-8062-baa4-cc05dd67dd3c" class="bulleted-list"><li style="list-style-type:disc">The attention mechanism operates in two main ways:<ul id="1b542ab5-d599-806c-ac27-eeb6bbd9f0f9" class="bulleted-list"><li style="list-style-type:circle"><strong>Spatial Attention</strong>: This module focuses on specific regions within each image, prioritizing areas that are more likely to contain changes (such as newly constructed buildings or roads). The spatial attention mechanism uses a weighted sum of the spatial features to highlight the most important spatial regions.</li></ul><ul id="1b542ab5-d599-80ed-bc84-ef6e19491470" class="bulleted-list"><li style="list-style-type:circle"><strong>Temporal Attention</strong>: This module captures the temporal differences between the two images in the pair. By learning the changes between the images, it assigns higher attention to regions that have undergone noticeable transformations over time.</li></ul></li></ul><ul id="1b542ab5-d599-802d-846f-c6e3fc4c2853" class="bulleted-list"><li style="list-style-type:disc">These two attention mechanisms allow STANet to effectively isolate the changes in the scene while ignoring irrelevant background areas. The attention maps generated by these modules guide the network to focus on areas of interest, improving the detection accuracy.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-8076-8ca8-fb2be0a6c130" class="numbered-list" start="4"><li><strong>Change Detection Module</strong>:<ul id="1b542ab5-d599-8017-a387-e898ec6d9f7d" class="bulleted-list"><li style="list-style-type:disc">After the attention modules have highlighted the relevant spatial and temporal features, the model performs the change detection task. This is typically done by computing the difference between the feature maps of the reference and target images, where changes are identified based on these differences.</li></ul><ul id="1b542ab5-d599-80a3-a444-d47f2f54caec" class="bulleted-list"><li style="list-style-type:disc">The output of this module is a binary map (or segmentation mask), where each pixel is classified as either a &quot;change&quot; or &quot;no-change&quot; based on the detected differences.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-806d-bb9a-e352d0d1b59d" class="numbered-list" start="5"><li><strong>Fusion Layer</strong>:<ul id="1b542ab5-d599-808d-b1d1-f22844d8367b" class="bulleted-list"><li style="list-style-type:disc">The features from both spatial and temporal attention modules are fused to provide a comprehensive representation of the changes. The fusion process combines the spatial and temporal information into a unified change map.</li></ul><ul id="1b542ab5-d599-8099-9637-f861240e0d26" class="bulleted-list"><li style="list-style-type:disc">Different fusion strategies can be applied, such as concatenation or element-wise summation, depending on the network design.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80c6-85d4-f9c784fe3834" class="numbered-list" start="6"><li><strong>Final Output</strong>:<ul id="1b542ab5-d599-80df-88ba-da5f9fd671ad" class="bulleted-list"><li style="list-style-type:disc">The final output is a change detection map, where each pixel is labeled as either &quot;changed&quot; or &quot;unchanged.&quot; This output can be further refined using post-processing techniques such as thresholding or morphological operations to improve the precision of the detected changes.</li></ul></li></ol><h3 id="1b542ab5-d599-8044-b98e-db6713a54f5b" class="">STANet&#x27;s Advantages</h3><ol type="1" id="1b542ab5-d599-805d-b7a4-fd18b417bdbd" class="numbered-list" start="1"><li><strong>Spatio-Temporal Focus</strong>:<ul id="1b542ab5-d599-805d-97d8-f4b5e454f60b" class="bulleted-list"><li style="list-style-type:disc">By using separate attention mechanisms for both spatial and temporal dimensions, STANet can capture complex patterns in satellite imagery that change over time, which is crucial for infrastructure change detection. This dual attention strategy allows the model to focus on both the location of the change and the temporal differences between two images.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80bf-b7da-f1dde4365043" class="numbered-list" start="2"><li><strong>Improved Accuracy</strong>:<ul id="1b542ab5-d599-80ef-93d4-fc338642e4dc" class="bulleted-list"><li style="list-style-type:disc">The attention mechanism helps the network ignore irrelevant regions and noisy data, making it more efficient and accurate in detecting subtle changes. The model can, for example, focus on urban development or infrastructure changes while ignoring stable regions like forests or water bodies.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-803a-9676-c2085f839d8b" class="numbered-list" start="3"><li><strong>Adaptability</strong>:<ul id="1b542ab5-d599-8037-ae73-e65111983622" class="bulleted-list"><li style="list-style-type:disc">STANet is highly adaptable to different remote sensing change detection tasks. It can be applied to a wide range of datasets, such as the Levir-CD dataset or others, to detect changes in urban areas, forests, agricultural land, or disaster-affected regions.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-8012-b379-f740abcb6d30" class="numbered-list" start="4"><li><strong>Efficient Feature Extraction</strong>:<ul id="1b542ab5-d599-80ba-8154-cc8f8be84884" class="bulleted-list"><li style="list-style-type:disc">By using deep convolutional layers, STANet can efficiently extract hierarchical features from both reference and target images. This deep feature extraction process helps the model learn complex patterns and subtle changes in the data, which traditional methods might miss.</li></ul></li></ol><h3 id="1b542ab5-d599-8039-b1d4-e81393eb3794" class="">Applications of STANet</h3><ul id="1b542ab5-d599-801c-b52a-f73903aa949b" class="bulleted-list"><li style="list-style-type:disc"><strong>Urban Monitoring</strong>: Detecting new buildings, roads, and infrastructure developments in urban areas over time.</li></ul><ul id="1b542ab5-d599-8051-bd84-fc6ae0e150ec" class="bulleted-list"><li style="list-style-type:disc"><strong>Disaster Management</strong>: Identifying damage caused by natural disasters (e.g., earthquakes, floods, or wildfires) by comparing satellite images before and after the event.</li></ul><ul id="1b542ab5-d599-80c5-bedb-e98056469d28" class="bulleted-list"><li style="list-style-type:disc"><strong>Environmental Monitoring</strong>: Tracking deforestation, land use changes, and vegetation health by comparing satellite images taken over different seasons or years.</li></ul><ul id="1b542ab5-d599-80b3-9dd4-fc7d6f27caf0" class="bulleted-list"><li style="list-style-type:disc"><strong>Military and Security</strong>: Monitoring changes in strategic locations or infrastructure that may indicate new developments or changes in an area of interest.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-8041-be9c-f2e247bf0882" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras import layers, models

# Define the Spatio-Temporal Attention Module
class SpatioTemporalAttention(layers.Layer):
    def __init__(self, filters):
        super(SpatioTemporalAttention, self).__init__()
        self.spatial_attention = layers.Conv2D(filters, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)
        self.temporal_attention = layers.Conv2D(filters, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)

    def call(self, input_ref, input_tar):
        # Spatial attention (focuses on relevant spatial features)
        spatial_attention_map = self.spatial_attention(input_ref - input_tar)
        
        # Temporal attention (focuses on differences over time)
        temporal_attention_map = self.temporal_attention(input_ref - input_tar)

        # Combine spatial and temporal attention maps
        attention_map = spatial_attention_map * temporal_attention_map
        
        return attention_map

# Define the STANet model architecture
def STANet(input_shape):
    input_ref = layers.Input(shape=input_shape)  # Input for reference image (time t1)
    input_tar = layers.Input(shape=input_shape)  # Input for target image (time t2)
    
    # Feature extraction using convolutional layers
    x_ref = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(input_ref)
    x_tar = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(input_tar)
    
    # Spatio-Temporal Attention Mechanism
    attention_module = SpatioTemporalAttention(64)
    attention_map = attention_module(x_ref, x_tar)
    
    # Apply attention map to the reference and target feature maps
    x_ref = layers.Multiply()([x_ref, attention_map])
    x_tar = layers.Multiply()([x_tar, attention_map])
    
    # Combine the features from both images
    x = layers.Concatenate()([x_ref, x_tar])
    
    # Further processing
    x = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
    x = layers.MaxPooling2D((2, 2))(x)
    
    # Decoder (upsampling to original resolution)
    x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
    x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
    
    # Output layer with binary classification (change/no-change)
    output = layers.Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(x)
    
    # Build the model
    model = models.Model(inputs=[input_ref, input_tar], outputs=output)
    
    return model

# Define input shape (e.g., 256x256x3 images)
input_shape = (256, 256, 3)

# Build and compile the STANet model
model = STANet(input_shape)
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Print the model summary
model.summary()
</code></pre><h2 id="1b542ab5-d599-80d4-b095-ccc6168adee7" class="">2. UNet: A Deep Learning Architecture for Image Segmentation</h2><p id="1b542ab5-d599-8081-83fc-fef474ee8344" class="">UNet is one of the most popular deep learning architectures for image segmentation tasks, and it has proven to be highly effective for change detection in satellite imagery. The architecture is named &quot;UNet&quot; because of its U-shaped structure, which consists of a contracting path (encoder) and an expansive path (decoder).</p><p id="1b542ab5-d599-80cf-8196-c80745138c5d" class="">In the encoder part of the UNet, the network uses a series of convolutional layers to extract features from the input images. These features are then passed through a bottleneck layer, after which the decoder part of the network upsamples the features to generate a pixel-wise segmentation mask. The decoder uses skip connections, which are direct links between the corresponding layers of the encoder and decoder. These skip connections allow the network to retain fine-grained spatial information that might be lost during the downsampling process in the encoder.</p><p id="1b542ab5-d599-8055-82e6-d7a88c37bd27" class="">For change detection, UNet can be applied to pairs of images from different time periods. The network learns to classify each pixel as either a &quot;change&quot; or &quot;no-change&quot; based on the differences between the two images. UNet has been used extensively for segmentation tasks in medical imaging, satellite imagery, and other domains due to its ability to provide precise pixel-level predictions.</p><h3 id="1b542ab5-d599-80dc-91ec-c11f68c07548" class="">U-Net Architecture Overview</h3><ul id="1b542ab5-d599-8006-a0dc-efbb822ca9ec" class="bulleted-list"><li style="list-style-type:disc"><strong>Encoder (Contracting Path)</strong>:<ul id="1b542ab5-d599-80e2-9084-d88d17983e3b" class="bulleted-list"><li style="list-style-type:circle">The encoder consists of a series of convolutional layers that downsample the image. It captures contextual information at different levels, progressively reducing spatial dimensions.</li></ul><ul id="1b542ab5-d599-80ec-a58f-ef7aa16aee19" class="bulleted-list"><li style="list-style-type:circle">Each convolutional block consists of two convolutional layers followed by max-pooling.</li></ul></li></ul><ul id="1b542ab5-d599-806a-8679-dd024d0a0dbb" class="bulleted-list"><li style="list-style-type:disc"><strong>Bottleneck</strong>:<ul id="1b542ab5-d599-80a8-bdcb-dbaaafba4ec7" class="bulleted-list"><li style="list-style-type:circle">The bottleneck sits at the deepest level of the network, where the spatial resolution is the smallest. It holds the most abstract features of the image.</li></ul></li></ul><ul id="1b542ab5-d599-803c-9fbc-d32a7a7810d7" class="bulleted-list"><li style="list-style-type:disc"><strong>Decoder (Expansive Path)</strong>:<ul id="1b542ab5-d599-80c2-8022-ed2cf6a6dc94" class="bulleted-list"><li style="list-style-type:circle">The decoder consists of a series of upsampling layers that restore the spatial dimensions of the image. It uses transposed convolutions (also known as deconvolutions) to increase the resolution.</li></ul></li></ul><ul id="1b542ab5-d599-80f5-9742-c310823d7b76" class="bulleted-list"><li style="list-style-type:disc"><strong>Skip Connections</strong>:<ul id="1b542ab5-d599-806f-88e4-f0a61d8503b6" class="bulleted-list"><li style="list-style-type:circle">U-Net has skip connections that connect the corresponding layers in the encoder and decoder paths. This allows the decoder to access high-resolution features from the encoder, which helps in accurately predicting pixel-level segmentation.</li></ul></li></ul><ul id="1b542ab5-d599-8054-84e4-e549ebee7ebd" class="bulleted-list"><li style="list-style-type:disc"><strong>Final Layer</strong>:<ul id="1b542ab5-d599-80f9-9b9b-e1ad274ad939" class="bulleted-list"><li style="list-style-type:circle">The final layer is a 1x1 convolution, which maps the feature map to the desired number of output channels (for binary segmentation, it outputs a single channel with values between 0 and 1 using a sigmoid function).</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-8020-aab3-fe7d96eecdfc" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras import layers, models

def UNet(input_shape):
    # Input layer
    inputs = layers.Input(shape=input_shape)

    # Encoder (Contracting Path)
    # Block 1
    x1 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(inputs)
    x1 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x1)
    p1 = layers.MaxPooling2D((2, 2))(x1)

    # Block 2
    x2 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p1)
    x2 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x2)
    p2 = layers.MaxPooling2D((2, 2))(x2)

    # Block 3
    x3 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p2)
    x3 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x3)
    p3 = layers.MaxPooling2D((2, 2))(x3)

    # Block 4
    x4 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p3)
    x4 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x4)
    p4 = layers.MaxPooling2D((2, 2))(x4)

    # Bottleneck (Deepest layer)
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p4)
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x5)

    # Decoder (Expansive Path)
    # Block 1
    u1 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(x5)
    u1 = layers.concatenate([u1, x4])  # Skip connection
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)

    # Block 2
    u2 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u1)
    u2 = layers.concatenate([u2, x3])  # Skip connection
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)

    # Block 3
    u3 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u2)
    u3 = layers.concatenate([u3, x2])  # Skip connection
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)

    # Block 4
    u4 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u3)
    u4 = layers.concatenate([u4, x1])  # Skip connection
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)

    # Output layer (1x1 convolution)
    outputs = layers.Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(u4)

    # Create the model
    model = models.Model(inputs=[inputs], outputs=[outputs])

    return model

# Define input shape (e.g., 256x256x3 images)
input_shape = (256, 256, 3)

# Build and compile the U-Net model
model = UNet(input_shape)
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Print model summary
model.summary()</code></pre><h2 id="1b542ab5-d599-8043-9185-dac6e5192861" class="">3. UNet with ResNet: Combining ResNet with UNet for Better Feature Extraction</h2><p id="1b542ab5-d599-8094-90da-e7f4a22fa30b" class="">ResNet (Residual Networks) is another deep learning architecture that has gained widespread popularity due to its ability to train very deep networks without suffering from vanishing gradients. ResNet achieves this by using skip connections that allow gradients to flow more easily through the network during training.</p><p id="1b542ab5-d599-80ff-b764-e8808f516762" class="">By combining ResNet with UNet, we can enhance the feature extraction capabilities of the original UNet model. ResNet provides a powerful backbone for feature extraction by utilizing residual blocks that learn to focus on important features while ignoring noise. These residual blocks help the model capture more complex patterns in the data, which is especially important when dealing with high-resolution satellite images.</p><p id="1b542ab5-d599-80fe-ad72-d3e85fc33fbd" class="">When applied to change detection, UNet with ResNet benefits from both the feature extraction power of ResNet and the segmentation capabilities of UNet. The result is a more accurate and robust model for detecting changes in satellite imagery. The combination of these two architectures allows the model to learn both high-level semantic features (such as buildings or roads) and low-level spatial features (such as textures and edges), which is crucial for detecting changes between two time points.</p><h3 id="1b542ab5-d599-803e-bb5a-d6e2ae2bce2b" class="">U-Net with ResNet Architecture:</h3><ol type="1" id="1b542ab5-d599-80cf-ba26-f70f0cecd643" class="numbered-list" start="1"><li><strong>Encoder (ResNet Backbone)</strong>:<ul id="1b542ab5-d599-8032-9cd3-ea22a785f45f" class="bulleted-list"><li style="list-style-type:disc">ResNet is used as a feature extractor in the encoder. It processes the input image and learns hierarchical features using residual blocks.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-8098-a7f5-fb8ac20f8f36" class="numbered-list" start="2"><li><strong>Skip Connections</strong>:<ul id="1b542ab5-d599-8032-bf11-ed1144628e5d" class="bulleted-list"><li style="list-style-type:disc">Skip connections from the encoder (ResNet) are added to the decoder to preserve high-resolution details that are important for segmentation tasks.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80c6-b604-f05eed93f786" class="numbered-list" start="3"><li><strong>Decoder (Upsampling Path)</strong>:<ul id="1b542ab5-d599-808e-9016-c366353b1f33" class="bulleted-list"><li style="list-style-type:disc">The decoder is a set of upsampling layers that gradually increase the image resolution and refine the segmentation prediction.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-807b-a5f9-c275e667242e" class="numbered-list" start="4"><li><strong>Final Layer</strong>:<ul id="1b542ab5-d599-8032-b917-ea0eb530f653" class="bulleted-list"><li style="list-style-type:disc">A <code>1x1</code> convolution layer outputs the final segmentation map (for binary segmentation, a sigmoid activation is used).</li></ul></li></ol><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-80d6-aeaa-d2cdacaee20d" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50

def UNet_ResNet(input_shape):
    # Input layer
    inputs = layers.Input(shape=input_shape)

    # ResNet50 as the encoder (pre-trained on ImageNet)
    resnet = ResNet50(weights=&#x27;imagenet&#x27;, include_top=False, input_tensor=inputs)
    
    # Encoder layers (ResNet layers with skip connections)
    x1 = resnet.get_layer(&#x27;conv1_relu&#x27;).output  # First ResNet block (Skip Connection 1)
    x2 = resnet.get_layer(&#x27;conv2_block3_out&#x27;).output  # Second ResNet block (Skip Connection 2)
    x3 = resnet.get_layer(&#x27;conv3_block4_out&#x27;).output  # Third ResNet block (Skip Connection 3)
    x4 = resnet.get_layer(&#x27;conv4_block6_out&#x27;).output  # Fourth ResNet block (Skip Connection 4)
    
    # Bottleneck layer
    x5 = resnet.output
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x5)
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x5)
    
    # Decoder (Expansive Path) with skip connections from ResNet
    u1 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(x5)
    u1 = layers.concatenate([u1, x4])  # Skip connection from ResNet block 4
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)

    u2 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u1)
    u2 = layers.concatenate([u2, x3])  # Skip connection from ResNet block 3
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)

    u3 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u2)
    u3 = layers.concatenate([u3, x2])  # Skip connection from ResNet block 2
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)

    u4 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u3)
    u4 = layers.concatenate([u4, x1])  # Skip connection from ResNet block 1
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)

    # Output layer (1x1 convolution)
    outputs = layers.Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(u4)

    # Create the model
    model = models.Model(inputs=[inputs], outputs=[outputs])

    return model

# Define input shape (e.g., 256x256x3 images)
input_shape = (256, 256, 3)

# Build and compile the U-Net with ResNet model
model = UNet_ResNet(input_shape)
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Print model summary
model.summary()
</code></pre><h2 id="1b542ab5-d599-8062-9cbd-dd3581f0f8c6" class="">4. LCDNet: Local Contextualized Deep Network for Change Detection</h2><p id="1b542ab5-d599-8015-a6a2-e31c58855fea" class="">LCDNet (Local Contextualized Deep Network) is a specialized architecture designed for change detection tasks in remote sensing images. It focuses on incorporating local contextual information to improve the model&#x27;s ability to detect changes in regions with high spatial variability.</p><p id="1b542ab5-d599-8000-8435-e2f706acbf37" class="">One of the main challenges in change detection is the presence of noise, variations in lighting, and other environmental factors that can lead to false positives or false negatives. LCDNet addresses this challenge by using a local contextualization technique that allows the network to focus on local regions of the image where changes are more likely to occur. This is particularly useful in urban environments, where infrastructure changes often happen in localized areas rather than across the entire scene.</p><p id="1b542ab5-d599-801c-a7dc-fd02db32b008" class="">The architecture of LCDNet includes convolutional layers for feature extraction, followed by a contextualization module that analyzes local regions to detect subtle changes. This approach improves the model&#x27;s sensitivity to small changes in the scene while minimizing the impact of noise and other irrelevant variations.</p><h3 id="1b542ab5-d599-8012-b0a7-df14159fe534" class="">LCDNet Architecture Overview</h3><ol type="1" id="1b542ab5-d599-80a9-bfc8-d2e38e667fa4" class="numbered-list" start="1"><li><strong>Input Layer</strong>:<ul id="1b542ab5-d599-80e2-8daf-e0ba07625060" class="bulleted-list"><li style="list-style-type:disc">The network takes two inputs, typically images from two different time periods, either as a pair of multi-spectral or RGB images (e.g., satellite images taken before and after a certain event).</li></ul></li></ol><ol type="1" id="1b542ab5-d599-802c-a00c-fc347cfad52e" class="numbered-list" start="2"><li><strong>Feature Extraction (Encoder)</strong>:<ul id="1b542ab5-d599-80e3-92cb-f2a38642c69e" class="bulleted-list"><li style="list-style-type:disc">The encoder extracts features from both the input images. It uses several convolutional layers to capture spatial features at various scales.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80c5-9925-ec50aa2ddf1c" class="numbered-list" start="3"><li><strong>Change Detection Layer</strong>:<ul id="1b542ab5-d599-801e-a92a-da88383c0528" class="bulleted-list"><li style="list-style-type:disc">A crucial part of LCDNet is the change detection layer, which highlights differences between the two input images. This layer computes pixel-wise differences and learns features that correspond to temporal changes (differences between the images).</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80f1-9293-dd0b4f32ee38" class="numbered-list" start="4"><li><strong>Decoder (Upsampling Path)</strong>:<ul id="1b542ab5-d599-803f-bc10-f8fb0a11d881" class="bulleted-list"><li style="list-style-type:disc">The decoder reconstructs the segmented output map. It typically consists of upsampling layers, which refine the prediction and output the change map (binary map) where each pixel indicates whether a change has occurred.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-8017-b3e3-ef69a63c0c53" class="numbered-list" start="5"><li><strong>Final Output</strong>:<ul id="1b542ab5-d599-809f-a013-eb6dee1b7c67" class="bulleted-list"><li style="list-style-type:disc">The final output is a binary segmentation map where pixels with a value of 1 indicate change, and 0 indicates no change.</li></ul></li></ol><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-8026-970d-f93b4a4d36e6" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50

def LCDNet(input_shape):
    # Input layers for two time periods (before and after images)
    input1 = layers.Input(shape=input_shape)  # Image at time t1
    input2 = layers.Input(shape=input_shape)  # Image at time t2
    
    # Encoder: Extract features from both images using shared convolutional layers
    encoder = models.Sequential()
    
    encoder.add(layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;, input_shape=input_shape))
    encoder.add(layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.MaxPooling2D((2, 2)))

    encoder.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.MaxPooling2D((2, 2)))

    encoder.add(layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.MaxPooling2D((2, 2)))
    
    # Extract features for both images
    features1 = encoder(input1)
    features2 = encoder(input2)

    # Change Detection Layer: Compute the pixel-wise difference between the two images
    change_map = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([features1, features2])

    # Decoder: Upsample the change map to the original image size
    decoder = models.Sequential()
    decoder.add(layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))

    # Output layer (binary segmentation map: change or no change)
    output = layers.Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(decoder(change_map))

    # Create the model
    model = models.Model(inputs=[input1, input2], outputs=[output])

    return model

# Define input shape (e.g., 256x256x3 images)
input_shape = (256, 256, 3)

# Build and compile the LCDNet model
model = LCDNet(input_shape)
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Print model summary
model.summary()</code></pre><p id="1b542ab5-d599-808d-b42b-fb593d99db55" class="">
</p><h2 id="1b542ab5-d599-807c-9adb-d9bfd8d3d603" class="">Results</h2><p id="1b542ab5-d599-80f6-be1a-d4f50a8df143" class="">STANet Performed well with an accuracy score of 98 Percent.</p><p id="1b542ab5-d599-802f-839b-dd00bf0da69d" class="">
</p><figure id="1b542ab5-d599-8088-be34-fb989243f462" class="image"><a href="__results___7_0.png"><img style="width:709.9874877929688px" src="__results___7_0.png"/></a></figure><h2 id="1b542ab5-d599-8089-a54e-fa811cc694ab" class="">Conclusion</h2><p id="1b542ab5-d599-8003-a14c-cb5e858e859a" class="">Change detection using deep learning models such as STANet, UNet, UNet with ResNet, and LCDNet has revolutionized the way we analyze satellite imagery for infrastructure changes. These models leverage advanced techniques like attention mechanisms, residual learning, and local contextualization to improve the accuracy and robustness of change detection tasks. By applying these models to datasets like Levir-CD, we can monitor urban growth, assess the impact of natural disasters, and detect other changes in infrastructure with high precision.</p><p id="1b542ab5-d599-809a-b88a-c68d2b1f02e5" class="">The Levir-CD dataset provides an excellent benchmark for training and evaluating these models, and the combination of deep learning architectures has shown impressive results in detecting changes in satellite images. As the field of remote sensing continues to evolve, we can expect these models to become even more effective, enabling real-time monitoring of global infrastructure changes and improving our ability to respond to environmental and urban challenges.</p><p id="1b542ab5-d599-8031-b1d1-def9aa924b2c" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>