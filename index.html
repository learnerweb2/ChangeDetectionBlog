<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Infrastructure Change Detection Using Satellite Imagery</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1b542ab5-d599-80db-b95e-f02a5252f932" class="page serif"><header><img class="page-cover-image" src="360_F_500840525_z0BVvB9i29AlWU8ZdMtFH0GewrLyBSob.jpg" style="object-position:center 48.870000000000005%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">üõ∞Ô∏è</span></div><h1 class="page-title">Infrastructure Change Detection Using Satellite Imagery</h1><p class="page-description"></p></header><div class="page-body"><p id="1b542ab5-d599-8142-8370-eb2d95818e66" class="block-color-gray">Team 117        -        Guide: MD. Sallaudin Sir</p><p id="1b542ab5-d599-804f-9c47-eea14c739fd2" class="">
</p><p id="1b542ab5-d599-8176-bef5-c3494939739c" class="block-color-gray"><mark class="highlight-default">THIPPANI VENKATA NAGASHESHU MANI - 2103A52036</mark></p><p id="1b542ab5-d599-8039-b940-d79d4ec1b98e" class="">UGGE REETHUVARMA                                              - 2103A52038</p><p id="1b542ab5-d599-80b8-9f73-d88f8261a821" class="">SATTU SAI PRANEETH                                               - 2103A52034 ¬†</p><p id="1b542ab5-d599-805a-9c0d-f966c7c136d3" class="">
</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1b542ab5-d599-8162-902a-c94ef6ff7237"><div style="font-size:1.5em"><span class="icon">üîñ</span></div><div style="width:100%">&quot;Deep learning-based infrastructure change detection using satellite imagery for accurate, automated monitoring and analysis of urban and rural developments.‚Äù</div></figure><h1 id="1b542ab5-d599-80c4-8cc0-ec8c9845a2e8" class="">Introduction</h1><p id="1b542ab5-d599-81a5-b73a-ec6687da23d3" class="">Infrastructure change detection using satellite imagery is a powerful tool that helps monitor changes in buildings, roads, and other structures over time. By using images captured by satellites, we can observe and track how areas change, whether due to construction, natural disasters, or urban development. This kind of analysis is important for urban planning, disaster response, environmental monitoring, and even assessing the impact of climate change.</p><div id="1b542ab5-d599-8131-a3ec-fa6f1d3e560c" class="column-list"><div id="1b542ab5-d599-81b7-8ed0-e3d07609f99f" style="width:50%" class="column"><p id="1b542ab5-d599-80a6-9fe8-efbde6dfde8b" class="">Deep learning, a type of artificial intelligence, plays a big role in this process. It involves training computer models to recognize patterns in satellite images and automatically detect changes. These models learn to differentiate between old and new buildings, roads, and other structures by analyzing large amounts of image data. This means the system can detect even small changes, such as a new building or road that was built recently.</p><p id="1b542ab5-d599-80a8-b769-ed1e97c1a9cc" class="">
</p></div><div id="1b542ab5-d599-813a-ab5a-d25b397ecede" style="width:50%" class="column"><figure id="1b542ab5-d599-81e4-8379-e350b91ff329" class="image"><a href="3419993.png"><img style="width:240px" src="3419993.png"/></a><figcaption>Satellite Imagery</figcaption></figure><p id="1b542ab5-d599-81f7-82d3-ceee98edbae7" class="block-color-gray">
</p></div></div><p id="1b542ab5-d599-819b-9511-cce5878c6e93" class="">Using deep learning for infrastructure change detection is faster and more accurate than traditional methods, which usually involve human analysis of images. With AI, we can detect changes in real-time and make important decisions quickly, like planning for new developments or responding to a disaster. This technology is becoming increasingly important as cities grow and as we need to monitor more areas than ever before.</p><p id="1b542ab5-d599-818b-a371-c72f68a97b22" class=""><br/>In this article, we will discuss four deep learning-based models for change detection: <br/><strong>STANet</strong>, <strong>UNet</strong>, <strong>UNet with ResNet</strong>, and <strong>LCDNet</strong>. Each of these models is designed to improve the performance of change detection tasks by leveraging different techniques for feature extraction, skip connections, and fusion of multi-temporal information.</p><h1 id="1b642ab5-d599-8059-9185-e02e7ef7e0a4" class="">Dataset</h1><p id="1b642ab5-d599-801b-9ae6-ca394aac400d" class=""><strong>The LEVIR-CD dataset is a large-scale, high-resolution collection of satellite images designed for building change detection tasks</strong></p><p id="1b642ab5-d599-80d0-901f-e39dce0fd123" class=""><strong>High-Resolution Dataset</strong> ‚Äì Contains <strong>637 pairs</strong> of <strong>1024 √ó 1024 pixel</strong> satellite images, with a resolution of <strong>0.5 meters per pixel</strong>.</p><p id="1b642ab5-d599-80d6-88eb-f8ad7f074f1a" class=""><strong>Focus on Building Changes</strong> ‚Äì Captures <strong>new building construction</strong> and <strong>demolition</strong> over a time span of <strong>5 to 14 years</strong>.</p><figure id="1b642ab5-d599-801d-9238-d5c43fd6034a" class="image"><a href="image.png"><img style="width:686px" src="image.png"/></a></figure><h1 id="1b642ab5-d599-80b8-84ef-d6d759db92a2" class="">Architecture</h1><p id="1b642ab5-d599-8096-8c5f-d5e741607ed8" class="">
</p><figure id="1b642ab5-d599-8000-ab80-ebb543f18a85" class="image"><a href="image%201.png"><img style="width:628px" src="image%201.png"/></a></figure><h1 id="1b642ab5-d599-80d9-8abb-e1bf9127d5d9" class="">Deep Learning Models</h1><h2 id="1b542ab5-d599-808c-a9c0-fa7966fd991d" class="">1. Siamese Unet</h2><p id="1b642ab5-d599-8003-8335-efd0e20f38d6" class="">Siamese U-Net is a deep learning model designed for image comparison tasks, such as change detection, similarity assessment, and medical image analysis. It combines the U-Net architecture with a Siamese network structure, enabling it to learn spatial and contextual features effectively. U-Net is well known for its encoder-decoder structure, which helps capture detailed spatial information, while the Siamese network utilizes two identical subnetworks with shared weights to compare two input images. By leveraging these strengths, Siamese U-Net provides high-accuracy segmentation and feature extraction, making it particularly useful in remote sensing, biomedical imaging, and infrastructure change detection applications.</p><hr id="1b642ab5-d599-80bb-a0ae-f8eef934d8a1"/><p id="1b642ab5-d599-8020-9af3-d02486c7d6dc" class=""><strong>Architecture of Siamese U-Net</strong></p><p id="1b642ab5-d599-80a3-bd97-eb540278fb86" class="">Siamese U-Net extends the U-Net architecture by incorporating a dual-branch design, where two identical U-Nets process different input images and extract relevant feature maps. The architecture consists of the following key components:</p><ol type="1" id="1b642ab5-d599-80bd-8864-e1d2134b14e1" class="numbered-list" start="1"><li><strong>Input Layer:</strong> Siamese U-Net takes two images as input, typically representing different time frames or different modalities.</li></ol><ol type="1" id="1b642ab5-d599-80dc-af1a-f2b5b16070b3" class="numbered-list" start="2"><li><strong>Shared Encoder:</strong> Each image passes through an encoder consisting of convolutional layers, batch normalization, and ReLU activation. The encoder extracts hierarchical feature representations.</li></ol><ol type="1" id="1b642ab5-d599-8058-8061-d0cf798bdc84" class="numbered-list" start="3"><li><strong>Bottleneck Layer:</strong> The deepest layer of the network acts as a feature fusion layer, where the feature maps from both branches are combined using operations like concatenation or subtraction.</li></ol><ol type="1" id="1b642ab5-d599-8040-b687-df9be65324c8" class="numbered-list" start="4"><li><strong>Decoder:</strong> A symmetric decoder reconstructs the spatial information using upsampling layers and skip connections, refining the feature maps for precise segmentation.</li></ol><ol type="1" id="1b642ab5-d599-80da-8174-e7d580f8b30b" class="numbered-list" start="5"><li><strong>Similarity Calculation:</strong> The final layer produces a similarity score or a segmentation map indicating differences between the two images.</li></ol><p id="1b642ab5-d599-8008-af2e-d8d7d2fc0cb4" class="">The shared encoder enables the model to learn robust features while maintaining computational efficiency. Skip connections from the encoder to the decoder improve feature retention and localization accuracy, ensuring precise boundary detection in change detection tasks.</p><p id="1b642ab5-d599-8042-9888-c68b458ba985" class="">This implementation consists of:</p><ul id="1b642ab5-d599-80fe-9348-e6a04fe39ae1" class="bulleted-list"><li style="list-style-type:disc"><strong>A shared encoder</strong> for feature extraction.</li></ul><ul id="1b642ab5-d599-8063-a44b-e949c33964ed" class="bulleted-list"><li style="list-style-type:disc"><strong>A bottleneck layer</strong> that fuses extracted features.</li></ul><ul id="1b642ab5-d599-8085-95ec-dcd1c27295b4" class="bulleted-list"><li style="list-style-type:disc"><strong>A decoder</strong> to reconstruct the spatial information.</li></ul><ul id="1b642ab5-d599-8091-baf5-c8c968fba80c" class="bulleted-list"><li style="list-style-type:disc"><strong>Final output</strong> using a sigmoid activation for binary segmentation.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-8041-be9c-f2e247bf0882" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation
from tensorflow.keras.models import Model

def conv_block(inputs, filters, kernel_size=(3,3), activation=&#x27;relu&#x27;):
    x = Conv2D(filters, kernel_size, padding=&#x27;same&#x27;)(inputs)
    x = BatchNormalization()(x)
    x = Activation(activation)(x)
    x = Conv2D(filters, kernel_size, padding=&#x27;same&#x27;)(x)
    x = BatchNormalization()(x)
    x = Activation(activation)(x)
    return x

def encoder_block(inputs, filters):
    x = conv_block(inputs, filters)
    p = MaxPooling2D((2,2))(x)
    return x, p

def decoder_block(inputs, skip_features, filters):
    x = UpSampling2D((2,2))(inputs)
    x = Concatenate()([x, skip_features])
    x = conv_block(x, filters)
    return x

def build_siamese_unet(input_shape=(256,256,3)):
    inputs1 = Input(input_shape)
    inputs2 = Input(input_shape)
    
    # Shared Encoder
    f1, p1 = encoder_block(inputs1, 64)
    f2, p2 = encoder_block(inputs2, 64)
    f3, p3 = encoder_block(p1, 128)
    f4, p4 = encoder_block(p2, 128)
    f5, p5 = encoder_block(p3, 256)
    f6, p6 = encoder_block(p4, 256)
    
    # Bottleneck
    b1 = conv_block(p5, 512)
    b2 = conv_block(p6, 512)
    fused = Concatenate()([b1, b2])
    
    # Decoder
    d1 = decoder_block(fused, f5, 256)
    d2 = decoder_block(d1, f3, 128)
    d3 = decoder_block(d2, f1, 64)
    
    outputs = Conv2D(1, (1,1), activation=&#x27;sigmoid&#x27;)(d3)
    
    model = Model([inputs1, inputs2], outputs, name=&#x27;Siamese_U-Net&#x27;)
    return model

# Create Model
model = build_siamese_unet()
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])
model.summary()</code></pre><p id="1b642ab5-d599-809a-ab45-c34e444f027b" class="">In conclusion, Siamese U-Net is a valuable model for change detection tasks, offering accuracy, efficiency, and adaptability across various domains. By implementing the provided code, researchers and developers can leverage this architecture for their specific applications, improving automation and precision in image analysis tasks.</p><h2 id="1b542ab5-d599-80d4-b095-ccc6168adee7" class="">2. UNet: A Deep Learning Architecture for Image Segmentation</h2><p id="1b542ab5-d599-8081-83fc-fef474ee8344" class="">UNet is one of the most popular deep learning architectures for image segmentation tasks, and it has proven to be highly effective for change detection in satellite imagery. The architecture is named &quot;UNet&quot; because of its U-shaped structure, which consists of a contracting path (encoder) and an expansive path (decoder).</p><p id="1b542ab5-d599-80cf-8196-c80745138c5d" class="">In the encoder part of the UNet, the network uses a series of convolutional layers to extract features from the input images. These features are then passed through a bottleneck layer, after which the decoder part of the network upsamples the features to generate a pixel-wise segmentation mask. The decoder uses skip connections, which are direct links between the corresponding layers of the encoder and decoder. These skip connections allow the network to retain fine-grained spatial information that might be lost during the downsampling process in the encoder.</p><p id="1b542ab5-d599-8055-82e6-d7a88c37bd27" class="">For change detection, UNet can be applied to pairs of images from different time periods. The network learns to classify each pixel as either a &quot;change&quot; or &quot;no-change&quot; based on the differences between the two images. UNet has been used extensively for segmentation tasks in medical imaging, satellite imagery, and other domains due to its ability to provide precise pixel-level predictions.</p><h3 id="1b542ab5-d599-80dc-91ec-c11f68c07548" class="">U-Net Architecture Overview</h3><ul id="1b542ab5-d599-8006-a0dc-efbb822ca9ec" class="bulleted-list"><li style="list-style-type:disc"><strong>Encoder (Contracting Path)</strong>:<ul id="1b542ab5-d599-80e2-9084-d88d17983e3b" class="bulleted-list"><li style="list-style-type:circle">The encoder consists of a series of convolutional layers that downsample the image. It captures contextual information at different levels, progressively reducing spatial dimensions.</li></ul><ul id="1b542ab5-d599-80ec-a58f-ef7aa16aee19" class="bulleted-list"><li style="list-style-type:circle">Each convolutional block consists of two convolutional layers followed by max-pooling.</li></ul></li></ul><ul id="1b542ab5-d599-806a-8679-dd024d0a0dbb" class="bulleted-list"><li style="list-style-type:disc"><strong>Bottleneck</strong>:<ul id="1b542ab5-d599-80a8-bdcb-dbaaafba4ec7" class="bulleted-list"><li style="list-style-type:circle">The bottleneck sits at the deepest level of the network, where the spatial resolution is the smallest. It holds the most abstract features of the image.</li></ul></li></ul><ul id="1b542ab5-d599-803c-9fbc-d32a7a7810d7" class="bulleted-list"><li style="list-style-type:disc"><strong>Decoder (Expansive Path)</strong>:<ul id="1b542ab5-d599-80c2-8022-ed2cf6a6dc94" class="bulleted-list"><li style="list-style-type:circle">The decoder consists of a series of upsampling layers that restore the spatial dimensions of the image. It uses transposed convolutions (also known as deconvolutions) to increase the resolution.</li></ul></li></ul><ul id="1b542ab5-d599-80f5-9742-c310823d7b76" class="bulleted-list"><li style="list-style-type:disc"><strong>Skip Connections</strong>:<ul id="1b542ab5-d599-806f-88e4-f0a61d8503b6" class="bulleted-list"><li style="list-style-type:circle">U-Net has skip connections that connect the corresponding layers in the encoder and decoder paths. This allows the decoder to access high-resolution features from the encoder, which helps in accurately predicting pixel-level segmentation.</li></ul></li></ul><ul id="1b542ab5-d599-8054-84e4-e549ebee7ebd" class="bulleted-list"><li style="list-style-type:disc"><strong>Final Layer</strong>:<ul id="1b542ab5-d599-80f9-9b9b-e1ad274ad939" class="bulleted-list"><li style="list-style-type:circle">The final layer is a 1x1 convolution, which maps the feature map to the desired number of output channels (for binary segmentation, it outputs a single channel with values between 0 and 1 using a sigmoid function).</li></ul></li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-8020-aab3-fe7d96eecdfc" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras import layers, models

def UNet(input_shape):
    # Input layer
    inputs = layers.Input(shape=input_shape)

    # Encoder (Contracting Path)
    # Block 1
    x1 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(inputs)
    x1 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x1)
    p1 = layers.MaxPooling2D((2, 2))(x1)

    # Block 2
    x2 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p1)
    x2 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x2)
    p2 = layers.MaxPooling2D((2, 2))(x2)

    # Block 3
    x3 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p2)
    x3 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x3)
    p3 = layers.MaxPooling2D((2, 2))(x3)

    # Block 4
    x4 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p3)
    x4 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x4)
    p4 = layers.MaxPooling2D((2, 2))(x4)

    # Bottleneck (Deepest layer)
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(p4)
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x5)

    # Decoder (Expansive Path)
    # Block 1
    u1 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(x5)
    u1 = layers.concatenate([u1, x4])  # Skip connection
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)

    # Block 2
    u2 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u1)
    u2 = layers.concatenate([u2, x3])  # Skip connection
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)

    # Block 3
    u3 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u2)
    u3 = layers.concatenate([u3, x2])  # Skip connection
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)

    # Block 4
    u4 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u3)
    u4 = layers.concatenate([u4, x1])  # Skip connection
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)

    # Output layer (1x1 convolution)
    outputs = layers.Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(u4)

    # Create the model
    model = models.Model(inputs=[inputs], outputs=[outputs])

    return model

# Define input shape (e.g., 256x256x3 images)
input_shape = (256, 256, 3)

# Build and compile the U-Net model
model = UNet(input_shape)
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Print model summary
model.summary()</code></pre><h2 id="1b542ab5-d599-8043-9185-dac6e5192861" class="">3. UNet with ResNet: Combining ResNet with UNet for Better Feature Extraction</h2><p id="1b542ab5-d599-8094-90da-e7f4a22fa30b" class="">ResNet (Residual Networks) is another deep learning architecture that has gained widespread popularity due to its ability to train very deep networks without suffering from vanishing gradients. ResNet achieves this by using skip connections that allow gradients to flow more easily through the network during training.</p><p id="1b542ab5-d599-80ff-b764-e8808f516762" class="">By combining ResNet with UNet, we can enhance the feature extraction capabilities of the original UNet model. ResNet provides a powerful backbone for feature extraction by utilizing residual blocks that learn to focus on important features while ignoring noise. These residual blocks help the model capture more complex patterns in the data, which is especially important when dealing with high-resolution satellite images.</p><p id="1b542ab5-d599-80fe-ad72-d3e85fc33fbd" class="">When applied to change detection, UNet with ResNet benefits from both the feature extraction power of ResNet and the segmentation capabilities of UNet. The result is a more accurate and robust model for detecting changes in satellite imagery. The combination of these two architectures allows the model to learn both high-level semantic features (such as buildings or roads) and low-level spatial features (such as textures and edges), which is crucial for detecting changes between two time points.</p><h3 id="1b542ab5-d599-803e-bb5a-d6e2ae2bce2b" class="">U-Net with ResNet Architecture:</h3><ol type="1" id="1b542ab5-d599-80cf-ba26-f70f0cecd643" class="numbered-list" start="1"><li><strong>Encoder (ResNet Backbone)</strong>:<ul id="1b542ab5-d599-8032-9cd3-ea22a785f45f" class="bulleted-list"><li style="list-style-type:disc">ResNet is used as a feature extractor in the encoder. It processes the input image and learns hierarchical features using residual blocks.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-8098-a7f5-fb8ac20f8f36" class="numbered-list" start="2"><li><strong>Skip Connections</strong>:<ul id="1b542ab5-d599-8032-bf11-ed1144628e5d" class="bulleted-list"><li style="list-style-type:disc">Skip connections from the encoder (ResNet) are added to the decoder to preserve high-resolution details that are important for segmentation tasks.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80c6-b604-f05eed93f786" class="numbered-list" start="3"><li><strong>Decoder (Upsampling Path)</strong>:<ul id="1b542ab5-d599-808e-9016-c366353b1f33" class="bulleted-list"><li style="list-style-type:disc">The decoder is a set of upsampling layers that gradually increase the image resolution and refine the segmentation prediction.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-807b-a5f9-c275e667242e" class="numbered-list" start="4"><li><strong>Final Layer</strong>:<ul id="1b542ab5-d599-8032-b917-ea0eb530f653" class="bulleted-list"><li style="list-style-type:disc">A <code>1x1</code> convolution layer outputs the final segmentation map (for binary segmentation, a sigmoid activation is used).</li></ul></li></ol><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-80d6-aeaa-d2cdacaee20d" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50

def UNet_ResNet(input_shape):
    # Input layer
    inputs = layers.Input(shape=input_shape)

    # ResNet50 as the encoder (pre-trained on ImageNet)
    resnet = ResNet50(weights=&#x27;imagenet&#x27;, include_top=False, input_tensor=inputs)
    
    # Encoder layers (ResNet layers with skip connections)
    x1 = resnet.get_layer(&#x27;conv1_relu&#x27;).output  # First ResNet block (Skip Connection 1)
    x2 = resnet.get_layer(&#x27;conv2_block3_out&#x27;).output  # Second ResNet block (Skip Connection 2)
    x3 = resnet.get_layer(&#x27;conv3_block4_out&#x27;).output  # Third ResNet block (Skip Connection 3)
    x4 = resnet.get_layer(&#x27;conv4_block6_out&#x27;).output  # Fourth ResNet block (Skip Connection 4)
    
    # Bottleneck layer
    x5 = resnet.output
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x5)
    x5 = layers.Conv2D(1024, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x5)
    
    # Decoder (Expansive Path) with skip connections from ResNet
    u1 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(x5)
    u1 = layers.concatenate([u1, x4])  # Skip connection from ResNet block 4
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)
    u1 = layers.Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u1)

    u2 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u1)
    u2 = layers.concatenate([u2, x3])  # Skip connection from ResNet block 3
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)
    u2 = layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u2)

    u3 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u2)
    u3 = layers.concatenate([u3, x2])  # Skip connection from ResNet block 2
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)
    u3 = layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u3)

    u4 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=&#x27;same&#x27;)(u3)
    u4 = layers.concatenate([u4, x1])  # Skip connection from ResNet block 1
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)
    u4 = layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(u4)

    # Output layer (1x1 convolution)
    outputs = layers.Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(u4)

    # Create the model
    model = models.Model(inputs=[inputs], outputs=[outputs])

    return model

# Define input shape (e.g., 256x256x3 images)
input_shape = (256, 256, 3)

# Build and compile the U-Net with ResNet model
model = UNet_ResNet(input_shape)
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Print model summary
model.summary()
</code></pre><h2 id="1b542ab5-d599-8062-9cbd-dd3581f0f8c6" class="">4. LCDNet: Local Contextualized Deep Network for Change Detection</h2><p id="1b542ab5-d599-8015-a6a2-e31c58855fea" class="">LCDNet (Local Contextualized Deep Network) is a specialized architecture designed for change detection tasks in remote sensing images. It focuses on incorporating local contextual information to improve the model&#x27;s ability to detect changes in regions with high spatial variability.</p><p id="1b542ab5-d599-8000-8435-e2f706acbf37" class="">One of the main challenges in change detection is the presence of noise, variations in lighting, and other environmental factors that can lead to false positives or false negatives. LCDNet addresses this challenge by using a local contextualization technique that allows the network to focus on local regions of the image where changes are more likely to occur. This is particularly useful in urban environments, where infrastructure changes often happen in localized areas rather than across the entire scene.</p><p id="1b542ab5-d599-801c-a7dc-fd02db32b008" class="">The architecture of LCDNet includes convolutional layers for feature extraction, followed by a contextualization module that analyzes local regions to detect subtle changes. This approach improves the model&#x27;s sensitivity to small changes in the scene while minimizing the impact of noise and other irrelevant variations.</p><h3 id="1b542ab5-d599-8012-b0a7-df14159fe534" class="">LCDNet Architecture Overview</h3><ol type="1" id="1b542ab5-d599-80a9-bfc8-d2e38e667fa4" class="numbered-list" start="1"><li><strong>Input Layer</strong>:<ul id="1b542ab5-d599-80e2-8daf-e0ba07625060" class="bulleted-list"><li style="list-style-type:disc">The network takes two inputs, typically images from two different time periods, either as a pair of multi-spectral or RGB images (e.g., satellite images taken before and after a certain event).</li></ul></li></ol><ol type="1" id="1b542ab5-d599-802c-a00c-fc347cfad52e" class="numbered-list" start="2"><li><strong>Feature Extraction (Encoder)</strong>:<ul id="1b542ab5-d599-80e3-92cb-f2a38642c69e" class="bulleted-list"><li style="list-style-type:disc">The encoder extracts features from both the input images. It uses several convolutional layers to capture spatial features at various scales.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80c5-9925-ec50aa2ddf1c" class="numbered-list" start="3"><li><strong>Change Detection Layer</strong>:<ul id="1b542ab5-d599-801e-a92a-da88383c0528" class="bulleted-list"><li style="list-style-type:disc">A crucial part of LCDNet is the change detection layer, which highlights differences between the two input images. This layer computes pixel-wise differences and learns features that correspond to temporal changes (differences between the images).</li></ul></li></ol><ol type="1" id="1b542ab5-d599-80f1-9293-dd0b4f32ee38" class="numbered-list" start="4"><li><strong>Decoder (Upsampling Path)</strong>:<ul id="1b542ab5-d599-803f-bc10-f8fb0a11d881" class="bulleted-list"><li style="list-style-type:disc">The decoder reconstructs the segmented output map. It typically consists of upsampling layers, which refine the prediction and output the change map (binary map) where each pixel indicates whether a change has occurred.</li></ul></li></ol><ol type="1" id="1b542ab5-d599-8017-b3e3-ef69a63c0c53" class="numbered-list" start="5"><li><strong>Final Output</strong>:<ul id="1b542ab5-d599-809f-a013-eb6dee1b7c67" class="bulleted-list"><li style="list-style-type:disc">The final output is a binary segmentation map where pixels with a value of 1 indicate change, and 0 indicates no change.</li></ul></li></ol><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b542ab5-d599-8026-970d-f93b4a4d36e6" class="code"><code class="language-Python">import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50

def LCDNet(input_shape):
    # Input layers for two time periods (before and after images)
    input1 = layers.Input(shape=input_shape)  # Image at time t1
    input2 = layers.Input(shape=input_shape)  # Image at time t2
    
    # Encoder: Extract features from both images using shared convolutional layers
    encoder = models.Sequential()
    
    encoder.add(layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;, input_shape=input_shape))
    encoder.add(layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.MaxPooling2D((2, 2)))

    encoder.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.MaxPooling2D((2, 2)))

    encoder.add(layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    encoder.add(layers.MaxPooling2D((2, 2)))
    
    # Extract features for both images
    features1 = encoder(input1)
    features2 = encoder(input2)

    # Change Detection Layer: Compute the pixel-wise difference between the two images
    change_map = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([features1, features2])

    # Decoder: Upsample the change map to the original image size
    decoder = models.Sequential()
    decoder.add(layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=&#x27;same&#x27;))
    decoder.add(layers.Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;))

    # Output layer (binary segmentation map: change or no change)
    output = layers.Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(decoder(change_map))

    # Create the model
    model = models.Model(inputs=[input1, input2], outputs=[output])

    return model

# Define input shape (e.g., 256x256x3 images)
input_shape = (256, 256, 3)

# Build and compile the LCDNet model
model = LCDNet(input_shape)
model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Print model summary
model.summary()</code></pre><p id="1b542ab5-d599-808d-b42b-fb593d99db55" class="">
</p><h2 id="1b542ab5-d599-807c-9adb-d9bfd8d3d603" class="">Results</h2><p id="1b542ab5-d599-80f6-be1a-d4f50a8df143" class="">STANet Performed well with an accuracy score of 98 Percent.</p><figure id="1b642ab5-d599-80ca-915c-c01b9c8ed618" class="image"><a href="image%202.png"><img style="width:709.9874877929688px" src="image%202.png"/></a></figure><figure id="1b642ab5-d599-80ff-9fc7-c298adc2d8d0" class="image"><a href="image%203.png"><img style="width:709.9249877929688px" src="image%203.png"/></a></figure><figure id="1b642ab5-d599-804c-9cca-f06967a8502f" class="image"><a href="image%204.png"><img style="width:709.9750366210938px" src="image%204.png"/></a></figure><figure id="1b542ab5-d599-8088-be34-fb989243f462" class="image"><a href="__results___7_0.png"><img style="width:709.9874877929688px" src="__results___7_0.png"/></a></figure><h2 id="1b542ab5-d599-8089-a54e-fa811cc694ab" class="">Conclusion</h2><p id="1b542ab5-d599-8003-a14c-cb5e858e859a" class="">Change detection using deep learning models such as STANet, UNet, UNet with ResNet, and LCDNet has revolutionized the way we analyze satellite imagery for infrastructure changes. These models leverage advanced techniques like attention mechanisms, residual learning, and local contextualization to improve the accuracy and robustness of change detection tasks. By applying these models to datasets like Levir-CD, we can monitor urban growth, assess the impact of natural disasters, and detect other changes in infrastructure with high precision.</p><p id="1b542ab5-d599-809a-b88a-c68d2b1f02e5" class="">The Levir-CD dataset provides an excellent benchmark for training and evaluating these models, and the combination of deep learning architectures has shown impressive results in detecting changes in satellite images. As the field of remote sensing continues to evolve, we can expect these models to become even more effective, enabling real-time monitoring of global infrastructure changes and improving our ability to respond to environmental and urban challenges.</p><p id="1b542ab5-d599-8031-b1d1-def9aa924b2c" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>